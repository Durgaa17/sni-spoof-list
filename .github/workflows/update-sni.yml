name: Auto-Update SNI List (ZeroTier/Free Data Focus)

on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC
  workflow_dispatch:  # Manual trigger

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          ref: main

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install requests beautifulsoup4 lxml

      - name: Scan & Update SNI List
        id: scan
        run: |
          python -c "
          import requests
          from bs4 import BeautifulSoup
          import re
          from datetime import datetime

          # Read existing list
          with open('sni-list.txt', 'r') as f:
              existing = set(line.strip() for line in f if line.strip() and not line.startswith('#'))

          new_domains = []

          # Scan 1: ZeroTier GitHub Issues
          resp = requests.get('https://api.github.com/repos/zerotier/ZeroTierOne/issues?state=closed&per_page=100')
          issues = resp.json()
          for issue in issues:
              title = issue['title'].lower()
              body = issue.get('body') or ''
              if 'free data' in title or 'malaysia' in title or 'unifi' in title:
                  domains = re.findall(r'([a-z0-9-]+\.[a-z]{2,}\.[a-z]{2,})', body)
                  new_domains.extend(domains)

          # Scan 2: Malaysia ISP Speedtest Pages
          isps = [
              'https://www.unifi.com.my/speedtest',
              'https://www.maxis.com.my/speedtest',
              'https://www.celcom.com.my/speedtest'
          ]
          for url in isps:
              try:
                  resp = requests.get(url, timeout=10)
                  soup = BeautifulSoup(resp.text, 'lxml')
                  links = [a.get('href') or '' for a in soup.find_all('a', href=True) if 'cdn' in (a.get('href') or '') or 'static' in (a.get('href') or '')]
                  domains = re.findall(r'([a-z0-9-]+\.[a-z]{2,}\.[a-z]{2,})', ' '.join(links))
                  new_domains.extend(domains)
              except:
                  pass

          # Scan 3: ZeroTier Reddit (free data threads)
          try:
              resp = requests.get('https://www.reddit.com/r/zerotier/search.json?q=free+data+malaysia&limit=10', headers={'User-Agent': 'PayloadMagic'})
              posts = resp.json().get('data', {}).get('children', [])
              for post in posts:
                  body = post.get('data', {}).get('selftext', '')
                  domains = re.findall(r'([a-z0-9-]+\.[a-z]{2,}\.[a-z]{2,})', body)
                  new_domains.extend(domains)
          except:
              pass

          # Filter & Add New
          new_unique = [d for d in set(new_domains) if d not in existing and d.endswith(('.com', '.net', '.my', '.org'))]
          if new_unique:
              with open('sni-list.txt', 'a') as f:
                  f.write('\n# Auto-Added ' + datetime.utcnow().isoformat() + '\n')
                  for d in new_unique:
                      f.write(d + '\n')
              print(f'Added {len(new_unique)} new domains: {new_unique[:5]}...')
          else:
              print('No new domains found')
          "

      - name: Commit & Push
        if: steps.scan.outcome == 'success'
        run: |
          git config user.name "PayloadMagic Bot"
          git config user.email "bot@payloadmagic.com"
          git add sni-list.txt
          if git diff --staged --quiet; then
            echo "No changes - skipping commit"
          else
            git commit -m "Auto-update SNI list: Added $(git diff --name-only HEAD~1 | wc -l) domains"
            git push origin HEAD:main
          fi

      - name: Notify on Failure
        if: failure()
        run: echo "Scan failed - check logs"
